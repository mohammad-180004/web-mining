{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Web Mining \u00b6 Profil Singkat \u00b6 Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Beranda"},{"location":"#selamat-datang-di-halaman-web-mining","text":"","title":"Selamat Datang di Halaman Web Mining"},{"location":"#profil-singkat","text":"Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Profil Singkat"},{"location":"crawling/","text":"Crawling \u00b6 Pengertian dari Crawling \u00b6 Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler Web Crawling dengan Requests-HTML \u00b6 Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin. 1. Memasang dan Memulai Library Requests-HTML \u00b6 Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession () 2. Memulai Crawling \u00b6 Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro. 3. Menyimpan Hasil Crawlin ke dalam File \u00b6 Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close () Web Crawling dengan Scrapy \u00b6 Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah: 1. Mempersiapkan Library Scrapy \u00b6 Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy 2. Melakukan Scraping pada Website \u00b6 Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja. Scrapy Shell \u00b6 Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy. Memulai Crawler \u00b6 Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah: Mengekstrak Konten dari Berita \u00b6 3. Membuat Spider Kustom \u00b6 4. Mengekspor Hasil Crawling \u00b6","title":"Web Crawling"},{"location":"crawling/#crawling","text":"","title":"Crawling"},{"location":"crawling/#pengertian-dari-crawling","text":"Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler","title":"Pengertian dari Crawling"},{"location":"crawling/#web-crawling-dengan-requests-html","text":"Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin.","title":"Web Crawling dengan Requests-HTML"},{"location":"crawling/#1-memasang-dan-memulai-library-requests-html","text":"Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession ()","title":"1. Memasang dan Memulai Library Requests-HTML"},{"location":"crawling/#2-memulai-crawling","text":"Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro.","title":"2. Memulai Crawling"},{"location":"crawling/#3-menyimpan-hasil-crawlin-ke-dalam-file","text":"Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close ()","title":"3. Menyimpan Hasil Crawlin ke dalam File"},{"location":"crawling/#web-crawling-dengan-scrapy","text":"Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah:","title":"Web Crawling dengan Scrapy"},{"location":"crawling/#1-mempersiapkan-library-scrapy","text":"Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy","title":"1. Mempersiapkan Library Scrapy"},{"location":"crawling/#2-melakukan-scraping-pada-website","text":"Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja.","title":"2. Melakukan Scraping pada Website"},{"location":"crawling/#scrapy-shell","text":"Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy.","title":"Scrapy Shell"},{"location":"crawling/#memulai-crawler","text":"Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah:","title":"Memulai Crawler"},{"location":"crawling/#mengekstrak-konten-dari-berita","text":"","title":"Mengekstrak Konten dari Berita"},{"location":"crawling/#3-membuat-spider-kustom","text":"","title":"3. Membuat Spider Kustom"},{"location":"crawling/#4-mengekspor-hasil-crawling","text":"","title":"4. Mengekspor Hasil Crawling"},{"location":"evaluasi/","text":"Evaluasi \u00b6","title":"LSA Evaluation"},{"location":"evaluasi/#evaluasi","text":"","title":"Evaluasi"},{"location":"modelling/","text":"Topic Modelling \u00b6 Topic Modelling dengan LSA \u00b6 Latent Semantic Analysis (LSA) adalah proses matematika yang berguna untuk mengklasifikasikan dan mengambil informasi menggunakan Singular Value Decomposition (SVD). LSA dapat digunakan untuk memahami hubungan antar kata agar lebih mudah dipahami. Menghitung LSA ini dapat dilakukan dengan data TF-IDF yang diperoleh sebelumya setelah preprocessing text. Mendapatkan Matriks TF-IDF \u00b6 Import Library \u00b6 import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.utils.extmath import randomized_svd Membaca Data \u00b6 textfile = codecs . open ( \"berita.txt\" , 'r' , 'utf-8' ) Article = \"\" for line in textfile : Article = Article + line print ( Article ) Preprocessing \u00b6 Melakukan tokenisasi dokumen = sent_tokenize ( Article ) Menghilangkan digit angka import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Menghilangkan stopword dari dokumen factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Menghilangkan tanda baca pada dokumen dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) Stemming untuk menghilangkan imbuhan pada kata factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Matriks TF-IDF \u00b6 Setelah preprocessing text, maka langkah selanjutnya adalah mendapatkan nilai matriks TF-IDF. vectorizer = TfidfVectorizer ( stop_words = stopwords , use_idf = True , ngram_range = ( 1 , 3 )) X = vectorizer . fit_transform ( dokumen ) X_T = np . transpose ( X ) Melakukan Metode LSA \u00b6 Mendapatkan Nilai SVD \u00b6 Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi atau fitur yang membantu untuk memperkecil nilai kompleksitas pada pemrosesan teks. SVD memiliki tiga matriks, diantaranya: Matriks ortogonal U Matriks diagonal S Transpose dari matriks ortogonal V Rumus dari SVD adalah: A = U \\Sigma V^T A = U \\Sigma V^T U , Sigma , VT = randomized_svd ( X_T , n_components = 100 , n_iter = 100 , random_state = None ) Mendapatkan Ringkasan Topik \u00b6 Setelah mendapatkan nilai dari SVD, maka selanjutnya adalah mendapat ringkasan topik dari dokumen. Berikut adalah cara mendapatkan ringkasan dari nilai SVD, dimana ringkasan topik ini diurutkan berdasarkan nilai SVD. Variabel k adalah berapa jumlah kalimat yang dijadikan sebagai ringkasan dari dokumen. k = 1 temp_k = k i = 0 index = 0 output = [] index_list = [] if k >= 3 : while k != 0 : if i == 0 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] index3 = dic_sort [ - 3 ][ 0 ] list = [ index1 , index2 , index3 ] list = sorted ( list ) for t in list : output . append ( dokumen [ t ]) index_list . append ( t ) k = k - 3 if k <= 0 : break elif i == 1 and temp_k != 4 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) temp_list = [] count = 0 for p in range ( - 1 , - len ( VT [ i ]), - 1 ): if count < 2 : if dic_sort [ p ][ 0 ] not in index_list : temp_list . append ( dic_sort [ p ][ 0 ]) index_list . append ( dic_sort [ p ][ 0 ]) list = sorted ( temp_list ) for t in list : output . append ( dokumen [ t ]) k = k - 2 if k <= 0 : break elif i >= 2 : max = - 9999999999 for j in range ( 0 , len ( VT [ i ])): if VT [ i ][ j ] > max and j not in index_list : index = j max = VT [ i ][ j ] index_list . append ( j ) output . append ( dokumen [ index ]) k = k - 1 i = i + 1 if temp_k < 3 : dic = {} for j in range ( 0 , len ( VT [ 0 ])): dic [ j ] = VT [ 0 ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] list = [ index1 , index2 ] list = sorted ( list ) if k == 1 : output . append ( dokumen [ list [ 0 ]]) if k == 2 : output . append ( dokumen [ list [ 0 ]]) output . append ( dokumen [ list [ 1 ]]) summarized_text = \" \" . join ( output ) print ( \"Hasil rangkuman: \\n \" , summarized_text )","title":"Topic Modelling"},{"location":"modelling/#topic-modelling","text":"","title":"Topic Modelling"},{"location":"modelling/#topic-modelling-dengan-lsa","text":"Latent Semantic Analysis (LSA) adalah proses matematika yang berguna untuk mengklasifikasikan dan mengambil informasi menggunakan Singular Value Decomposition (SVD). LSA dapat digunakan untuk memahami hubungan antar kata agar lebih mudah dipahami. Menghitung LSA ini dapat dilakukan dengan data TF-IDF yang diperoleh sebelumya setelah preprocessing text.","title":"Topic Modelling dengan LSA"},{"location":"modelling/#mendapatkan-matriks-tf-idf","text":"","title":"Mendapatkan Matriks TF-IDF"},{"location":"modelling/#import-library","text":"import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.utils.extmath import randomized_svd","title":"Import Library"},{"location":"modelling/#membaca-data","text":"textfile = codecs . open ( \"berita.txt\" , 'r' , 'utf-8' ) Article = \"\" for line in textfile : Article = Article + line print ( Article )","title":"Membaca Data"},{"location":"modelling/#preprocessing","text":"Melakukan tokenisasi dokumen = sent_tokenize ( Article ) Menghilangkan digit angka import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Menghilangkan stopword dari dokumen factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Menghilangkan tanda baca pada dokumen dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) Stemming untuk menghilangkan imbuhan pada kata factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem )","title":"Preprocessing"},{"location":"modelling/#matriks-tf-idf","text":"Setelah preprocessing text, maka langkah selanjutnya adalah mendapatkan nilai matriks TF-IDF. vectorizer = TfidfVectorizer ( stop_words = stopwords , use_idf = True , ngram_range = ( 1 , 3 )) X = vectorizer . fit_transform ( dokumen ) X_T = np . transpose ( X )","title":"Matriks TF-IDF"},{"location":"modelling/#melakukan-metode-lsa","text":"","title":"Melakukan Metode LSA"},{"location":"modelling/#mendapatkan-nilai-svd","text":"Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi atau fitur yang membantu untuk memperkecil nilai kompleksitas pada pemrosesan teks. SVD memiliki tiga matriks, diantaranya: Matriks ortogonal U Matriks diagonal S Transpose dari matriks ortogonal V Rumus dari SVD adalah: A = U \\Sigma V^T A = U \\Sigma V^T U , Sigma , VT = randomized_svd ( X_T , n_components = 100 , n_iter = 100 , random_state = None )","title":"Mendapatkan Nilai SVD"},{"location":"modelling/#mendapatkan-ringkasan-topik","text":"Setelah mendapatkan nilai dari SVD, maka selanjutnya adalah mendapat ringkasan topik dari dokumen. Berikut adalah cara mendapatkan ringkasan dari nilai SVD, dimana ringkasan topik ini diurutkan berdasarkan nilai SVD. Variabel k adalah berapa jumlah kalimat yang dijadikan sebagai ringkasan dari dokumen. k = 1 temp_k = k i = 0 index = 0 output = [] index_list = [] if k >= 3 : while k != 0 : if i == 0 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] index3 = dic_sort [ - 3 ][ 0 ] list = [ index1 , index2 , index3 ] list = sorted ( list ) for t in list : output . append ( dokumen [ t ]) index_list . append ( t ) k = k - 3 if k <= 0 : break elif i == 1 and temp_k != 4 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) temp_list = [] count = 0 for p in range ( - 1 , - len ( VT [ i ]), - 1 ): if count < 2 : if dic_sort [ p ][ 0 ] not in index_list : temp_list . append ( dic_sort [ p ][ 0 ]) index_list . append ( dic_sort [ p ][ 0 ]) list = sorted ( temp_list ) for t in list : output . append ( dokumen [ t ]) k = k - 2 if k <= 0 : break elif i >= 2 : max = - 9999999999 for j in range ( 0 , len ( VT [ i ])): if VT [ i ][ j ] > max and j not in index_list : index = j max = VT [ i ][ j ] index_list . append ( j ) output . append ( dokumen [ index ]) k = k - 1 i = i + 1 if temp_k < 3 : dic = {} for j in range ( 0 , len ( VT [ 0 ])): dic [ j ] = VT [ 0 ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] list = [ index1 , index2 ] list = sorted ( list ) if k == 1 : output . append ( dokumen [ list [ 0 ]]) if k == 2 : output . append ( dokumen [ list [ 0 ]]) output . append ( dokumen [ list [ 1 ]]) summarized_text = \" \" . join ( output ) print ( \"Hasil rangkuman: \\n \" , summarized_text )","title":"Mendapatkan Ringkasan Topik"},{"location":"preprocessing/","text":"Preprocessing Text \u00b6 Data yang baru saja di-crawl, mungkin masih belum dalam bentuk yang terstruktur (sembarang), sehingga data tersebut masih perlu dilakukan \"Preprocessing Text\". Preprocessing Text adalah proses yang mengubah sebuah data yang tidak beraturan menjadi data yang terstruktur. Preprocessing text tidak memiliki tahapan yang pasti, dikarenakan jenis data yang berbeda membutuhkan aturan pemrosesan yang berbeda. Namun, tahapan yang umumnya digunakan pada pemrosesan teks adalah Parsing, Tokenizer (memecah data menjadi kalimat-kalimat), Stopword Removal (menghilangkan kata-kata yang tidak memiliki informasi penting), dan Stemming (menghilangkan imbuhan pada kata). Melakukan Preprocessing Text \u00b6 Library yang Dibutuhkan \u00b6 Library yang dibutuhkan pada preprocessing text kali ini adalah: - Sastrawi (untuk teks Bahasa Indonesia) - Requests-HTML (jika menggunakan library ini untuk crawling) - NLTK (untuk tokenizer dan stopword removal jika teks berbahasa Inggris) - Scikit-learn Menyiapkan Dokumen \u00b6 Langkah pertama untuk melakukan pemrosesan teks adalah memanggil data dari file atau hasil crawling. Memanggil file eksternal textfile = codecs . open ( \"filename.txt\" , 'r' , 'utf-8' ) news = \"\" for line in textfile : news = news + line Menggunakan hasil crawler yang tersedia from requests_html import HTMLSession session = HTMLSession () r = session . get ( 'link-of-website' ) articles = r . html . find ( 'class' ) for item in articles : newsitem = item . find ( 'sub-class' , first = True ) news = newsitem . text print ( news ) Tokenizing \u00b6 Langkah selanjutnya adalah melakukan tokenizing. Tokenizing adalah membagi sebuah dokumen teks menjadi kalimat-kalimat dan menyimpannya ke dalam sebuah list. doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( news ) Maka hasilnya akan menjadi seperti berikut ini: Regular Expression \u00b6 Langkah ini untuk menghilangkan angka dan elemen Python \"\\n\" pada teks dokumen. Baris kode di bawah ini untuk menghilangkan digit angka pada dokumen: import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Setelah menghilangkan angka, maka langkah selanjutnya menghilangkan elemen \"\\n\" pada list kalimat dokumen. Baris kode di bawah ini untuk menghilangkan elemen tersebut: dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen ) Menghilangkan Stopword \u00b6 Stopword adalah kata-kata yang diabaikan dalam pemrosesan teks, karena stopword adalah kata-kata yang tidak memberikan informasi penting. Contoh dari stopword pada Bahasa Indonesia adalah kata penghubung seperti \"atau\", \"tapi\", \"dan\", dan lain sebagainya. Untuk menghilangkan stopword Bahasa Indonesia, library Python yang digunakan adalah Sastrawi. Dengan mengurangi stopword, maka jumlah kata pada dokumen akan lebih sedikit (dan jumlah fitur juga lebih sedikit) untuk pemrosesan teks. from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Stemming \u00b6 Stemming adalah proses untuk mentransformasikan sebuah kata dengan menghilangkan imbuhan (seperti \"me-\", \"-kan\") dan menjadi sebuah kata dasar. Misalnya, kata \"mengajarkan\" menjadi kata dasar \"ajar\". Untuk teks Bahasa Indonesia, proses stemming menggunakan Sastrawi: import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Langkah selanjutnya adalah menghilangkan tanda baca dalam kalimat: from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) print ( dokumenstop ) Matriks TF-IDF \u00b6 Term Frequency-Inverse Document Frequency (TF-IDF) adalah matriks yang mengukur seberapa penting istilah atau fitur pada suatu dokumen. Matriks TF-IDF ini digunakan untuk pemodelan topik. Vector Space Model \u00b6 Vector space model adalah from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer cv = CountVectorizer () cv_matrix = cv . fit_transform ( dokumen ) a = cv_matrix . toarray () bag = cv . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () matrik_vsm [ 0 ] Mendapatkan nama fitur: a = cv . get_feature_names () print ( len ( matrik_vsm [:, 1 ])) Matriks VSM: import pandas as pd dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ]) Matriks TF-IDF \u00b6 from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( cv . fit_transform ( dokumenstem )) . toarray () dfb = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ])","title":"Preprocessing Text"},{"location":"preprocessing/#preprocessing-text","text":"Data yang baru saja di-crawl, mungkin masih belum dalam bentuk yang terstruktur (sembarang), sehingga data tersebut masih perlu dilakukan \"Preprocessing Text\". Preprocessing Text adalah proses yang mengubah sebuah data yang tidak beraturan menjadi data yang terstruktur. Preprocessing text tidak memiliki tahapan yang pasti, dikarenakan jenis data yang berbeda membutuhkan aturan pemrosesan yang berbeda. Namun, tahapan yang umumnya digunakan pada pemrosesan teks adalah Parsing, Tokenizer (memecah data menjadi kalimat-kalimat), Stopword Removal (menghilangkan kata-kata yang tidak memiliki informasi penting), dan Stemming (menghilangkan imbuhan pada kata).","title":"Preprocessing Text"},{"location":"preprocessing/#melakukan-preprocessing-text","text":"","title":"Melakukan Preprocessing Text"},{"location":"preprocessing/#library-yang-dibutuhkan","text":"Library yang dibutuhkan pada preprocessing text kali ini adalah: - Sastrawi (untuk teks Bahasa Indonesia) - Requests-HTML (jika menggunakan library ini untuk crawling) - NLTK (untuk tokenizer dan stopword removal jika teks berbahasa Inggris) - Scikit-learn","title":"Library yang Dibutuhkan"},{"location":"preprocessing/#menyiapkan-dokumen","text":"Langkah pertama untuk melakukan pemrosesan teks adalah memanggil data dari file atau hasil crawling. Memanggil file eksternal textfile = codecs . open ( \"filename.txt\" , 'r' , 'utf-8' ) news = \"\" for line in textfile : news = news + line Menggunakan hasil crawler yang tersedia from requests_html import HTMLSession session = HTMLSession () r = session . get ( 'link-of-website' ) articles = r . html . find ( 'class' ) for item in articles : newsitem = item . find ( 'sub-class' , first = True ) news = newsitem . text print ( news )","title":"Menyiapkan Dokumen"},{"location":"preprocessing/#tokenizing","text":"Langkah selanjutnya adalah melakukan tokenizing. Tokenizing adalah membagi sebuah dokumen teks menjadi kalimat-kalimat dan menyimpannya ke dalam sebuah list. doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( news ) Maka hasilnya akan menjadi seperti berikut ini:","title":"Tokenizing"},{"location":"preprocessing/#regular-expression","text":"Langkah ini untuk menghilangkan angka dan elemen Python \"\\n\" pada teks dokumen. Baris kode di bawah ini untuk menghilangkan digit angka pada dokumen: import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Setelah menghilangkan angka, maka langkah selanjutnya menghilangkan elemen \"\\n\" pada list kalimat dokumen. Baris kode di bawah ini untuk menghilangkan elemen tersebut: dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen )","title":"Regular Expression"},{"location":"preprocessing/#menghilangkan-stopword","text":"Stopword adalah kata-kata yang diabaikan dalam pemrosesan teks, karena stopword adalah kata-kata yang tidak memberikan informasi penting. Contoh dari stopword pada Bahasa Indonesia adalah kata penghubung seperti \"atau\", \"tapi\", \"dan\", dan lain sebagainya. Untuk menghilangkan stopword Bahasa Indonesia, library Python yang digunakan adalah Sastrawi. Dengan mengurangi stopword, maka jumlah kata pada dokumen akan lebih sedikit (dan jumlah fitur juga lebih sedikit) untuk pemrosesan teks. from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop )","title":"Menghilangkan Stopword"},{"location":"preprocessing/#stemming","text":"Stemming adalah proses untuk mentransformasikan sebuah kata dengan menghilangkan imbuhan (seperti \"me-\", \"-kan\") dan menjadi sebuah kata dasar. Misalnya, kata \"mengajarkan\" menjadi kata dasar \"ajar\". Untuk teks Bahasa Indonesia, proses stemming menggunakan Sastrawi: import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Langkah selanjutnya adalah menghilangkan tanda baca dalam kalimat: from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) print ( dokumenstop )","title":"Stemming"},{"location":"preprocessing/#matriks-tf-idf","text":"Term Frequency-Inverse Document Frequency (TF-IDF) adalah matriks yang mengukur seberapa penting istilah atau fitur pada suatu dokumen. Matriks TF-IDF ini digunakan untuk pemodelan topik.","title":"Matriks TF-IDF"},{"location":"preprocessing/#vector-space-model","text":"Vector space model adalah from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer cv = CountVectorizer () cv_matrix = cv . fit_transform ( dokumen ) a = cv_matrix . toarray () bag = cv . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () matrik_vsm [ 0 ] Mendapatkan nama fitur: a = cv . get_feature_names () print ( len ( matrik_vsm [:, 1 ])) Matriks VSM: import pandas as pd dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ])","title":"Vector Space Model"},{"location":"preprocessing/#matriks-tf-idf_1","text":"from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( cv . fit_transform ( dokumenstem )) . toarray () dfb = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ])","title":"Matriks TF-IDF"}]}