{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Web Mining \u00b6 Profil Singkat \u00b6 Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Beranda"},{"location":"#selamat-datang-di-halaman-web-mining","text":"","title":"Selamat Datang di Halaman Web Mining"},{"location":"#profil-singkat","text":"Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Profil Singkat"},{"location":"crawling/","text":"Crawling \u00b6 Pengertian dari Crawling \u00b6 Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler Web Crawling dengan Requests-HTML \u00b6 Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin. 1. Memasang dan Memulai Library Requests-HTML \u00b6 Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession () 2. Memulai Crawling \u00b6 Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro. 3. Menyimpan Hasil Crawlin ke dalam File \u00b6 Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close () Web Crawling dengan Scrapy \u00b6 Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah: 1. Mempersiapkan Library Scrapy \u00b6 Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy 2. Melakukan Scraping pada Website \u00b6 Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja. Scrapy Shell \u00b6 Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy. Memulai Crawler \u00b6 Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah: Mengekstrak Konten dari Berita \u00b6 3. Membuat Spider Kustom \u00b6 4. Mengekspor Hasil Crawling \u00b6","title":"Web Crawling"},{"location":"crawling/#crawling","text":"","title":"Crawling"},{"location":"crawling/#pengertian-dari-crawling","text":"Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler","title":"Pengertian dari Crawling"},{"location":"crawling/#web-crawling-dengan-requests-html","text":"Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin.","title":"Web Crawling dengan Requests-HTML"},{"location":"crawling/#1-memasang-dan-memulai-library-requests-html","text":"Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession ()","title":"1. Memasang dan Memulai Library Requests-HTML"},{"location":"crawling/#2-memulai-crawling","text":"Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro.","title":"2. Memulai Crawling"},{"location":"crawling/#3-menyimpan-hasil-crawlin-ke-dalam-file","text":"Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close ()","title":"3. Menyimpan Hasil Crawlin ke dalam File"},{"location":"crawling/#web-crawling-dengan-scrapy","text":"Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah:","title":"Web Crawling dengan Scrapy"},{"location":"crawling/#1-mempersiapkan-library-scrapy","text":"Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy","title":"1. Mempersiapkan Library Scrapy"},{"location":"crawling/#2-melakukan-scraping-pada-website","text":"Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja.","title":"2. Melakukan Scraping pada Website"},{"location":"crawling/#scrapy-shell","text":"Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy.","title":"Scrapy Shell"},{"location":"crawling/#memulai-crawler","text":"Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah:","title":"Memulai Crawler"},{"location":"crawling/#mengekstrak-konten-dari-berita","text":"","title":"Mengekstrak Konten dari Berita"},{"location":"crawling/#3-membuat-spider-kustom","text":"","title":"3. Membuat Spider Kustom"},{"location":"crawling/#4-mengekspor-hasil-crawling","text":"","title":"4. Mengekspor Hasil Crawling"},{"location":"evaluasi/","text":"Evaluasi Ringkasan Otomatis \u00b6 Recall-Oriented Understudy for Gisting Evaluation (ROUGE) adalah perhitungan evaluasi ringkasan dari teks atau dokumen. ROUGE bekerja dengan membandingkan ringkasan yang dibuat secara otomatis atau terjemahan dengan ringkasan rujukan. Berikut ini adalah contoh dari ringkasan referensi dan ringkasan mesin: Ringkasan Referensi The cat was under the bed Ringkasan Mesin The cat was found under the bed N-Gram \u00b6 N-gram adalah pengelompokan token/kata. Misalnya, unigram (1-gram) akan terdiri dari satu kata dan bigram (2-gram) terdiri dari dua kata yang berurutan. Berikut ini contoh dari n-gram: Unigram (1-gram) [\"the\", \"cat\", \"was\", \"under\", \"the\", \"bed\"] Bigram (2-gram) [\"the cat\", \"cat was\", \"was under\", \"under the\", \"the bed\"] Trigram (3-gram) [\"the cat was\", \"cat was under\", \"was under the\", \"under the bed\"] Jenis-Jenis Metode dari ROUGE \u00b6 Ada bermacam-macam dari metode ROUGE, diantaranya: ROUGE-N \u00b6 ROUGE-N menghitung jumlah \"n-gram\" yang cocok antara ringkasan yang dibuat mesin dengan ringkasan rujukan. ROUGE-L \u00b6 Menghitung urutan terpanjang kata yang cocok menggunakan Longest Common Subsequence (LCS). Kelebihan menggunakan LCS adalah tidak memerlukan kecocokan yang berturut-turut tapi kecocokan pada urutan yang merefleksikan urutan kata pada tingkatan kalimat. Karena itu secara otomatis memasukkan n-gram umum terpanjang di dalam urutan, maka tidak perlu mendefinisikan terlebih dahulu untuk panjang dari n-gram. ROUGE-S \u00b6 ROUGE-S adalah segala pasangan kata yang berurutan di dalam sebuah kalimat, memungkinkan untuk celah sembarangan. ROUGE-S juga dapat disebut sebagai Skip-gram Coocurrence. Misalnya, skip-bigram menghitung jumlah pasangan kata yang overlap yang dapat memiliki dua celah maksimum antar kata. Contohnya, untuk kalimat \"cat in the hat\" maka skip-gram-nya adalah \"cat in, cat the, cat hat, in the, in hat, the hat\". ROUGE-W \u00b6 ROUGE-SU \u00b6 Karena ROUGE-S hanya memperhatikan bigram, maka dikembangkanlah metode ROGUE-SU yang juga memperhatikan unigram. Jika sebuah kalimat tidak memiliki kata yang overlap, maka tidak ada bobot yang dihasilkan pada kalimat. Recall, Precision, dan F-measure \u00b6 Recall \u00b6 Recall menghitung jumlah n-grams yang overlap ditemukan pada ringkasan mesin dan rujukan dan dibagi dengan jumlah kata pada ringkasan rujukan. Rumus dari Recall adalah: $$ R = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan rujukan}} $$ Recall bagus untuk memastikan ringkasan yang dibuat mendapatkan semua informasi yang terkandung dalam rujukan, tapi recall tidak terlalu bagus untuk memastikan ringkasan yang dibuat tidak mendorong jumlah kata yang sangat besar ke dalam skor recall. Contoh dari penghitungan Recall: Ringkasan mesin: \"The cat was found under the bed\" Ringkasan rujukan: \"The cat was under the bed\" Kata yang overlap: \"The\", \"cat\", \"was\", \"under\", \"the\", \"bed\" Maka nilai recall-nya adalah: $$ Recall = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan rujukan}} $$ Recall = \\frac{6}{6} = 100\\text{%} Recall = \\frac{6}{6} = 100\\text{%} Precission \u00b6 Untuk menghindari kelemahan pada recall, maka precision digunakan. Dimana dihitung dengan cara yang hampir sama, tapi daripada dibagi jumlah n-grams rujukan, jumlah kata yang overlap dibagi dengan jumlah n-gram pada mesin. Rumus dari Precission adalah: $$ P = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan mesin}} $$ Contoh dari penghitungan Precision: Ringkasan mesin: \"The cat was found under the bed\" Ringkasan rujukan: \"The cat was under the bed\" Kata yang overlap: \"The\", \"cat\", \"was\", \"under\", \"the\", \"bed\" Maka nilai precision-nya adalah: $$ Precission = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan mesin}} $$ Precission = \\frac{6}{7} = 85.7\\text{%} = 86\\text{%} Precission = \\frac{6}{7} = 85.7\\text{%} = 86\\text{%} F-Measure \u00b6 F-measure memberikan informasi lengkap berkaitan dengan recall dan precision. Rumus dari F-mesure adalah: $$ F-measure = \\frac{(1 + \\beta^2) R \\times P}{R + \\beta^2 \\times P} $$ Jika \\beta = 1 \\beta = 1 , maka: $$ F-measure = \\frac{ 2 \\times R \\times P}{R + P} $$ Implementasi ROUGE \u00b6 Memasang Library Python \u00b6 Ada dua cara untuk memasang library untuk menghitung ROUGE pada Python: - Memasang library dari GitHub: pip install git+https://github.com/tagucci/pythonrouge.git Memasang library menggunakan PyPI pip install easy-rouge Impementasi 1 \u00b6 from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing' . split () reference_sentence = 'Beijing is the capital of China' . split () # Calculate ROUGE-2. recall , precision , rouge = rouge_n_sentence_level ( summary_sentence , reference_sentence , 2 ) print ( 'ROUGE-2-R' , recall ) print ( 'ROUGE-2-P' , precision ) print ( 'ROUGE-2-F' , rouge ) Maka hasilnya adalah: ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6 Implementasi 2 \u00b6 from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_w_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_summary_level reference_sentence = 'the police killed the gunman' . split () summary_sentence = 'the gunman police killed' . split () print ( 'Sentence level:' ) score = rouge_n_sentence_level ( summary_sentence , reference_sentence , 1 ) print ( 'ROUGE-1: %f ' % score . f1_measure ) _ , _ , rouge_2 = rouge_n_sentence_level ( summary_sentence , reference_sentence , 2 ) print ( 'ROUGE-2: %f ' % rouge_2 ) _ , _ , rouge_l = rouge_l_sentence_level ( summary_sentence , reference_sentence ) print ( 'ROUGE-L: %f ' % rouge_l ) _ , _ , rouge_w = rouge_w_sentence_level ( summary_sentence , reference_sentence ) print ( 'ROUGE-W: %f ' % rouge_w ) Maka hasil output-nya adalah: Sentence level: ROUGE-1: 0.888889 ROUGE-2: 0.571429 ROUGE-L: 0.666667 ROUGE-W: 0.550527 Contoh lainnya adalah: from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentences = [ 'The gunman was shot dead by the police before more people got hurt' . split (), 'This tragedy causes lives of five , the gunman included' . split (), 'The motivation of the gunman remains unclear' . split (), ] summary_sentences = [ 'Police killed the gunman . no more people got hurt '.split(), 'Five people got killed including the gunman' . split (), 'It is unclear why the gunman killed people' . split (), ] print ( 'Summary level:' ) _ , _ , rouge_1 = rouge_n_summary_level ( summary_sentences , reference_sentences , 1 ) print ( 'ROUGE-1: %f ' % rouge_1 ) _ , _ , rouge_2 = rouge_n_summary_level ( summary_sentences , reference_sentences , 2 ) print ( 'ROUGE-2: %f ' % rouge_2 ) _ , _ , rouge_l = rouge_l_summary_level ( summary_sentences , reference_sentences ) print ( 'ROUGE-L: %f ' % rouge_l ) _ , _ , rouge_w = rouge_w_summary_level ( summary_sentences , reference_sentences ) print ( 'ROUGE-W: %f ' % rouge_w ) Outputnya adalah: Summary level: ROUGE-1: 0.400000 ROUGE-2: 0.188679 ROUGE-L: 0.327273 ROUGE-W: 0.200430 recall , precision , rouge_1 = rouge_n_summary_level ( summary_sentences , reference_sentences , 1 ) print ( 'ROUGE-2-R' , recall ) print ( 'ROUGE-2-P' , precision ) print ( 'ROUGE-2-F' , rouge ) Outputnya adalah: ROUGE-2-R 0.36666666666666664 ROUGE-2-P 0.44 ROUGE-2-F 0.","title":"Summarization Evaluation"},{"location":"evaluasi/#evaluasi-ringkasan-otomatis","text":"Recall-Oriented Understudy for Gisting Evaluation (ROUGE) adalah perhitungan evaluasi ringkasan dari teks atau dokumen. ROUGE bekerja dengan membandingkan ringkasan yang dibuat secara otomatis atau terjemahan dengan ringkasan rujukan. Berikut ini adalah contoh dari ringkasan referensi dan ringkasan mesin: Ringkasan Referensi The cat was under the bed Ringkasan Mesin The cat was found under the bed","title":"Evaluasi Ringkasan Otomatis"},{"location":"evaluasi/#n-gram","text":"N-gram adalah pengelompokan token/kata. Misalnya, unigram (1-gram) akan terdiri dari satu kata dan bigram (2-gram) terdiri dari dua kata yang berurutan. Berikut ini contoh dari n-gram: Unigram (1-gram) [\"the\", \"cat\", \"was\", \"under\", \"the\", \"bed\"] Bigram (2-gram) [\"the cat\", \"cat was\", \"was under\", \"under the\", \"the bed\"] Trigram (3-gram) [\"the cat was\", \"cat was under\", \"was under the\", \"under the bed\"]","title":"N-Gram"},{"location":"evaluasi/#jenis-jenis-metode-dari-rouge","text":"Ada bermacam-macam dari metode ROUGE, diantaranya:","title":"Jenis-Jenis Metode dari ROUGE"},{"location":"evaluasi/#rouge-n","text":"ROUGE-N menghitung jumlah \"n-gram\" yang cocok antara ringkasan yang dibuat mesin dengan ringkasan rujukan.","title":"ROUGE-N"},{"location":"evaluasi/#rouge-l","text":"Menghitung urutan terpanjang kata yang cocok menggunakan Longest Common Subsequence (LCS). Kelebihan menggunakan LCS adalah tidak memerlukan kecocokan yang berturut-turut tapi kecocokan pada urutan yang merefleksikan urutan kata pada tingkatan kalimat. Karena itu secara otomatis memasukkan n-gram umum terpanjang di dalam urutan, maka tidak perlu mendefinisikan terlebih dahulu untuk panjang dari n-gram.","title":"ROUGE-L"},{"location":"evaluasi/#rouge-s","text":"ROUGE-S adalah segala pasangan kata yang berurutan di dalam sebuah kalimat, memungkinkan untuk celah sembarangan. ROUGE-S juga dapat disebut sebagai Skip-gram Coocurrence. Misalnya, skip-bigram menghitung jumlah pasangan kata yang overlap yang dapat memiliki dua celah maksimum antar kata. Contohnya, untuk kalimat \"cat in the hat\" maka skip-gram-nya adalah \"cat in, cat the, cat hat, in the, in hat, the hat\".","title":"ROUGE-S"},{"location":"evaluasi/#rouge-w","text":"","title":"ROUGE-W"},{"location":"evaluasi/#rouge-su","text":"Karena ROUGE-S hanya memperhatikan bigram, maka dikembangkanlah metode ROGUE-SU yang juga memperhatikan unigram. Jika sebuah kalimat tidak memiliki kata yang overlap, maka tidak ada bobot yang dihasilkan pada kalimat.","title":"ROUGE-SU"},{"location":"evaluasi/#recall-precision-dan-f-measure","text":"","title":"Recall, Precision, dan F-measure"},{"location":"evaluasi/#recall","text":"Recall menghitung jumlah n-grams yang overlap ditemukan pada ringkasan mesin dan rujukan dan dibagi dengan jumlah kata pada ringkasan rujukan. Rumus dari Recall adalah: $$ R = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan rujukan}} $$ Recall bagus untuk memastikan ringkasan yang dibuat mendapatkan semua informasi yang terkandung dalam rujukan, tapi recall tidak terlalu bagus untuk memastikan ringkasan yang dibuat tidak mendorong jumlah kata yang sangat besar ke dalam skor recall. Contoh dari penghitungan Recall: Ringkasan mesin: \"The cat was found under the bed\" Ringkasan rujukan: \"The cat was under the bed\" Kata yang overlap: \"The\", \"cat\", \"was\", \"under\", \"the\", \"bed\" Maka nilai recall-nya adalah: $$ Recall = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan rujukan}} $$ Recall = \\frac{6}{6} = 100\\text{%} Recall = \\frac{6}{6} = 100\\text{%}","title":"Recall"},{"location":"evaluasi/#precission","text":"Untuk menghindari kelemahan pada recall, maka precision digunakan. Dimana dihitung dengan cara yang hampir sama, tapi daripada dibagi jumlah n-grams rujukan, jumlah kata yang overlap dibagi dengan jumlah n-gram pada mesin. Rumus dari Precission adalah: $$ P = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan mesin}} $$ Contoh dari penghitungan Precision: Ringkasan mesin: \"The cat was found under the bed\" Ringkasan rujukan: \"The cat was under the bed\" Kata yang overlap: \"The\", \"cat\", \"was\", \"under\", \"the\", \"bed\" Maka nilai precision-nya adalah: $$ Precission = \\frac{\\text{Banyaknya kata yang overlap}}{\\text{Banyaknya kata pada ringkasan mesin}} $$ Precission = \\frac{6}{7} = 85.7\\text{%} = 86\\text{%} Precission = \\frac{6}{7} = 85.7\\text{%} = 86\\text{%}","title":"Precission"},{"location":"evaluasi/#f-measure","text":"F-measure memberikan informasi lengkap berkaitan dengan recall dan precision. Rumus dari F-mesure adalah: $$ F-measure = \\frac{(1 + \\beta^2) R \\times P}{R + \\beta^2 \\times P} $$ Jika \\beta = 1 \\beta = 1 , maka: $$ F-measure = \\frac{ 2 \\times R \\times P}{R + P} $$","title":"F-Measure"},{"location":"evaluasi/#implementasi-rouge","text":"","title":"Implementasi ROUGE"},{"location":"evaluasi/#memasang-library-python","text":"Ada dua cara untuk memasang library untuk menghitung ROUGE pada Python: - Memasang library dari GitHub: pip install git+https://github.com/tagucci/pythonrouge.git Memasang library menggunakan PyPI pip install easy-rouge","title":"Memasang Library Python"},{"location":"evaluasi/#impementasi-1","text":"from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing' . split () reference_sentence = 'Beijing is the capital of China' . split () # Calculate ROUGE-2. recall , precision , rouge = rouge_n_sentence_level ( summary_sentence , reference_sentence , 2 ) print ( 'ROUGE-2-R' , recall ) print ( 'ROUGE-2-P' , precision ) print ( 'ROUGE-2-F' , rouge ) Maka hasilnya adalah: ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6","title":"Impementasi 1"},{"location":"evaluasi/#implementasi-2","text":"from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_w_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_summary_level reference_sentence = 'the police killed the gunman' . split () summary_sentence = 'the gunman police killed' . split () print ( 'Sentence level:' ) score = rouge_n_sentence_level ( summary_sentence , reference_sentence , 1 ) print ( 'ROUGE-1: %f ' % score . f1_measure ) _ , _ , rouge_2 = rouge_n_sentence_level ( summary_sentence , reference_sentence , 2 ) print ( 'ROUGE-2: %f ' % rouge_2 ) _ , _ , rouge_l = rouge_l_sentence_level ( summary_sentence , reference_sentence ) print ( 'ROUGE-L: %f ' % rouge_l ) _ , _ , rouge_w = rouge_w_sentence_level ( summary_sentence , reference_sentence ) print ( 'ROUGE-W: %f ' % rouge_w ) Maka hasil output-nya adalah: Sentence level: ROUGE-1: 0.888889 ROUGE-2: 0.571429 ROUGE-L: 0.666667 ROUGE-W: 0.550527 Contoh lainnya adalah: from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentences = [ 'The gunman was shot dead by the police before more people got hurt' . split (), 'This tragedy causes lives of five , the gunman included' . split (), 'The motivation of the gunman remains unclear' . split (), ] summary_sentences = [ 'Police killed the gunman . no more people got hurt '.split(), 'Five people got killed including the gunman' . split (), 'It is unclear why the gunman killed people' . split (), ] print ( 'Summary level:' ) _ , _ , rouge_1 = rouge_n_summary_level ( summary_sentences , reference_sentences , 1 ) print ( 'ROUGE-1: %f ' % rouge_1 ) _ , _ , rouge_2 = rouge_n_summary_level ( summary_sentences , reference_sentences , 2 ) print ( 'ROUGE-2: %f ' % rouge_2 ) _ , _ , rouge_l = rouge_l_summary_level ( summary_sentences , reference_sentences ) print ( 'ROUGE-L: %f ' % rouge_l ) _ , _ , rouge_w = rouge_w_summary_level ( summary_sentences , reference_sentences ) print ( 'ROUGE-W: %f ' % rouge_w ) Outputnya adalah: Summary level: ROUGE-1: 0.400000 ROUGE-2: 0.188679 ROUGE-L: 0.327273 ROUGE-W: 0.200430 recall , precision , rouge_1 = rouge_n_summary_level ( summary_sentences , reference_sentences , 1 ) print ( 'ROUGE-2-R' , recall ) print ( 'ROUGE-2-P' , precision ) print ( 'ROUGE-2-F' , rouge ) Outputnya adalah: ROUGE-2-R 0.36666666666666664 ROUGE-2-P 0.44 ROUGE-2-F 0.","title":"Implementasi 2"},{"location":"modelling/","text":"Topic Modelling \u00b6 Topic Modelling dengan LSA \u00b6 Latent Semantic Analysis (LSA) adalah proses matematika yang berguna untuk mengklasifikasikan dan mengambil informasi menggunakan Singular Value Decomposition (SVD). LSA dapat digunakan untuk memahami hubungan antar kata agar lebih mudah dipahami. Menghitung LSA ini dapat dilakukan dengan data TF-IDF yang diperoleh sebelumya setelah preprocessing text. Mendapatkan Matriks TF-IDF \u00b6 Import Library \u00b6 import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.utils.extmath import randomized_svd Membaca Data \u00b6 textfile = codecs . open ( \"berita.txt\" , 'r' , 'utf-8' ) Article = \"\" for line in textfile : Article = Article + line print ( Article ) Preprocessing \u00b6 Melakukan tokenisasi dokumen = sent_tokenize ( Article ) Menghilangkan digit angka import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Menghilangkan stopword dari dokumen factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Menghilangkan tanda baca pada dokumen dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) Stemming untuk menghilangkan imbuhan pada kata factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Matriks TF-IDF \u00b6 Setelah preprocessing text, maka langkah selanjutnya adalah mendapatkan nilai matriks TF-IDF. vectorizer = TfidfVectorizer ( stop_words = stopwords , use_idf = True , ngram_range = ( 1 , 3 )) X = vectorizer . fit_transform ( dokumen ) X_T = np . transpose ( X ) Melakukan Metode LSA \u00b6 Mendapatkan Nilai SVD \u00b6 Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi atau fitur yang membantu untuk memperkecil nilai kompleksitas pada pemrosesan teks. SVD memiliki tiga matriks, diantaranya: Matriks ortogonal U Matriks diagonal S Transpose dari matriks ortogonal V Rumus dari SVD adalah: A = U \\Sigma V^T A = U \\Sigma V^T U , Sigma , VT = randomized_svd ( X_T , n_components = 100 , n_iter = 100 , random_state = None ) Mendapatkan Ringkasan Topik \u00b6 Setelah mendapatkan nilai dari SVD, maka selanjutnya adalah mendapat ringkasan topik dari dokumen. Berikut adalah cara mendapatkan ringkasan dari nilai SVD, dimana ringkasan topik ini diurutkan berdasarkan nilai SVD. Variabel k adalah berapa jumlah kalimat yang dijadikan sebagai ringkasan dari dokumen. k = 1 temp_k = k i = 0 index = 0 output = [] index_list = [] if k >= 3 : while k != 0 : if i == 0 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] index3 = dic_sort [ - 3 ][ 0 ] list = [ index1 , index2 , index3 ] list = sorted ( list ) for t in list : output . append ( dokumen [ t ]) index_list . append ( t ) k = k - 3 if k <= 0 : break elif i == 1 and temp_k != 4 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) temp_list = [] count = 0 for p in range ( - 1 , - len ( VT [ i ]), - 1 ): if count < 2 : if dic_sort [ p ][ 0 ] not in index_list : temp_list . append ( dic_sort [ p ][ 0 ]) index_list . append ( dic_sort [ p ][ 0 ]) list = sorted ( temp_list ) for t in list : output . append ( dokumen [ t ]) k = k - 2 if k <= 0 : break elif i >= 2 : max = - 9999999999 for j in range ( 0 , len ( VT [ i ])): if VT [ i ][ j ] > max and j not in index_list : index = j max = VT [ i ][ j ] index_list . append ( j ) output . append ( dokumen [ index ]) k = k - 1 i = i + 1 if temp_k < 3 : dic = {} for j in range ( 0 , len ( VT [ 0 ])): dic [ j ] = VT [ 0 ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] list = [ index1 , index2 ] list = sorted ( list ) if k == 1 : output . append ( dokumen [ list [ 0 ]]) if k == 2 : output . append ( dokumen [ list [ 0 ]]) output . append ( dokumen [ list [ 1 ]]) summarized_text = \" \" . join ( output ) print ( \"Hasil rangkuman: \\n \" , summarized_text )","title":"Topic Modelling"},{"location":"modelling/#topic-modelling","text":"","title":"Topic Modelling"},{"location":"modelling/#topic-modelling-dengan-lsa","text":"Latent Semantic Analysis (LSA) adalah proses matematika yang berguna untuk mengklasifikasikan dan mengambil informasi menggunakan Singular Value Decomposition (SVD). LSA dapat digunakan untuk memahami hubungan antar kata agar lebih mudah dipahami. Menghitung LSA ini dapat dilakukan dengan data TF-IDF yang diperoleh sebelumya setelah preprocessing text.","title":"Topic Modelling dengan LSA"},{"location":"modelling/#mendapatkan-matriks-tf-idf","text":"","title":"Mendapatkan Matriks TF-IDF"},{"location":"modelling/#import-library","text":"import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.utils.extmath import randomized_svd","title":"Import Library"},{"location":"modelling/#membaca-data","text":"textfile = codecs . open ( \"berita.txt\" , 'r' , 'utf-8' ) Article = \"\" for line in textfile : Article = Article + line print ( Article )","title":"Membaca Data"},{"location":"modelling/#preprocessing","text":"Melakukan tokenisasi dokumen = sent_tokenize ( Article ) Menghilangkan digit angka import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Menghilangkan stopword dari dokumen factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Menghilangkan tanda baca pada dokumen dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) Stemming untuk menghilangkan imbuhan pada kata factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem )","title":"Preprocessing"},{"location":"modelling/#matriks-tf-idf","text":"Setelah preprocessing text, maka langkah selanjutnya adalah mendapatkan nilai matriks TF-IDF. vectorizer = TfidfVectorizer ( stop_words = stopwords , use_idf = True , ngram_range = ( 1 , 3 )) X = vectorizer . fit_transform ( dokumen ) X_T = np . transpose ( X )","title":"Matriks TF-IDF"},{"location":"modelling/#melakukan-metode-lsa","text":"","title":"Melakukan Metode LSA"},{"location":"modelling/#mendapatkan-nilai-svd","text":"Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi atau fitur yang membantu untuk memperkecil nilai kompleksitas pada pemrosesan teks. SVD memiliki tiga matriks, diantaranya: Matriks ortogonal U Matriks diagonal S Transpose dari matriks ortogonal V Rumus dari SVD adalah: A = U \\Sigma V^T A = U \\Sigma V^T U , Sigma , VT = randomized_svd ( X_T , n_components = 100 , n_iter = 100 , random_state = None )","title":"Mendapatkan Nilai SVD"},{"location":"modelling/#mendapatkan-ringkasan-topik","text":"Setelah mendapatkan nilai dari SVD, maka selanjutnya adalah mendapat ringkasan topik dari dokumen. Berikut adalah cara mendapatkan ringkasan dari nilai SVD, dimana ringkasan topik ini diurutkan berdasarkan nilai SVD. Variabel k adalah berapa jumlah kalimat yang dijadikan sebagai ringkasan dari dokumen. k = 1 temp_k = k i = 0 index = 0 output = [] index_list = [] if k >= 3 : while k != 0 : if i == 0 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] index3 = dic_sort [ - 3 ][ 0 ] list = [ index1 , index2 , index3 ] list = sorted ( list ) for t in list : output . append ( dokumen [ t ]) index_list . append ( t ) k = k - 3 if k <= 0 : break elif i == 1 and temp_k != 4 : dic = {} for j in range ( 0 , len ( VT [ i ])): dic [ j ] = VT [ i ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) temp_list = [] count = 0 for p in range ( - 1 , - len ( VT [ i ]), - 1 ): if count < 2 : if dic_sort [ p ][ 0 ] not in index_list : temp_list . append ( dic_sort [ p ][ 0 ]) index_list . append ( dic_sort [ p ][ 0 ]) list = sorted ( temp_list ) for t in list : output . append ( dokumen [ t ]) k = k - 2 if k <= 0 : break elif i >= 2 : max = - 9999999999 for j in range ( 0 , len ( VT [ i ])): if VT [ i ][ j ] > max and j not in index_list : index = j max = VT [ i ][ j ] index_list . append ( j ) output . append ( dokumen [ index ]) k = k - 1 i = i + 1 if temp_k < 3 : dic = {} for j in range ( 0 , len ( VT [ 0 ])): dic [ j ] = VT [ 0 ][ j ] dic_sort = sorted ( dic . items (), key = operator . itemgetter ( 1 )) index1 = dic_sort [ - 1 ][ 0 ] index2 = dic_sort [ - 2 ][ 0 ] list = [ index1 , index2 ] list = sorted ( list ) if k == 1 : output . append ( dokumen [ list [ 0 ]]) if k == 2 : output . append ( dokumen [ list [ 0 ]]) output . append ( dokumen [ list [ 1 ]]) summarized_text = \" \" . join ( output ) print ( \"Hasil rangkuman: \\n \" , summarized_text )","title":"Mendapatkan Ringkasan Topik"},{"location":"preprocessing/","text":"Preprocessing Text \u00b6 Data yang baru saja di-crawl, mungkin masih belum dalam bentuk yang terstruktur (sembarang), sehingga data tersebut masih perlu dilakukan \"Preprocessing Text\". Preprocessing Text adalah proses yang mengubah sebuah data yang tidak beraturan menjadi data yang terstruktur. Preprocessing text tidak memiliki tahapan yang pasti, dikarenakan jenis data yang berbeda membutuhkan aturan pemrosesan yang berbeda. Namun, tahapan yang umumnya digunakan pada pemrosesan teks adalah Parsing, Tokenizer (memecah data menjadi kalimat-kalimat), Stopword Removal (menghilangkan kata-kata yang tidak memiliki informasi penting), dan Stemming (menghilangkan imbuhan pada kata). Melakukan Preprocessing Text \u00b6 Library yang Dibutuhkan \u00b6 Library yang dibutuhkan pada preprocessing text kali ini adalah: - Sastrawi (untuk teks Bahasa Indonesia) - Requests-HTML (jika menggunakan library ini untuk crawling) - NLTK (untuk tokenizer dan stopword removal jika teks berbahasa Inggris) - Scikit-learn Menyiapkan Dokumen \u00b6 Langkah pertama untuk melakukan pemrosesan teks adalah memanggil data dari file atau hasil crawling. Memanggil file eksternal textfile = codecs . open ( \"filename.txt\" , 'r' , 'utf-8' ) news = \"\" for line in textfile : news = news + line Menggunakan hasil crawler yang tersedia from requests_html import HTMLSession session = HTMLSession () r = session . get ( 'link-of-website' ) articles = r . html . find ( 'class' ) for item in articles : newsitem = item . find ( 'sub-class' , first = True ) news = newsitem . text print ( news ) Tokenizing \u00b6 Langkah selanjutnya adalah melakukan tokenizing. Tokenizing adalah membagi sebuah dokumen teks menjadi kalimat-kalimat dan menyimpannya ke dalam sebuah list. doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( news ) Maka hasilnya akan menjadi seperti berikut ini: Regular Expression \u00b6 Langkah ini untuk menghilangkan angka dan elemen Python \"\\n\" pada teks dokumen. Baris kode di bawah ini untuk menghilangkan digit angka pada dokumen: import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Setelah menghilangkan angka, maka langkah selanjutnya menghilangkan elemen \"\\n\" pada list kalimat dokumen. Baris kode di bawah ini untuk menghilangkan elemen tersebut: dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen ) Menghilangkan Stopword \u00b6 Stopword adalah kata-kata yang diabaikan dalam pemrosesan teks, karena stopword adalah kata-kata yang tidak memberikan informasi penting. Contoh dari stopword pada Bahasa Indonesia adalah kata penghubung seperti \"atau\", \"tapi\", \"dan\", dan lain sebagainya. Untuk menghilangkan stopword Bahasa Indonesia, library Python yang digunakan adalah Sastrawi. Dengan mengurangi stopword, maka jumlah kata pada dokumen akan lebih sedikit (dan jumlah fitur juga lebih sedikit) untuk pemrosesan teks. from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) Stemming \u00b6 Stemming adalah proses untuk mentransformasikan sebuah kata dengan menghilangkan imbuhan (seperti \"me-\", \"-kan\") dan menjadi sebuah kata dasar. Misalnya, kata \"mengajarkan\" menjadi kata dasar \"ajar\". Untuk teks Bahasa Indonesia, proses stemming menggunakan Sastrawi: import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Langkah selanjutnya adalah menghilangkan tanda baca dalam kalimat: from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) print ( dokumenstop ) Matriks TF-IDF \u00b6 Term Frequency-Inverse Document Frequency (TF-IDF) adalah matriks yang mengukur seberapa penting istilah atau fitur pada suatu dokumen. Matriks TF-IDF ini digunakan untuk pemodelan topik. Vector Space Model \u00b6 Vector space model adalah from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer cv = CountVectorizer () cv_matrix = cv . fit_transform ( dokumen ) a = cv_matrix . toarray () bag = cv . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () matrik_vsm [ 0 ] Mendapatkan nama fitur: a = cv . get_feature_names () print ( len ( matrik_vsm [:, 1 ])) Matriks VSM: import pandas as pd dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ]) Matriks TF-IDF \u00b6 from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( cv . fit_transform ( dokumenstem )) . toarray () dfb = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ])","title":"Preprocessing Text"},{"location":"preprocessing/#preprocessing-text","text":"Data yang baru saja di-crawl, mungkin masih belum dalam bentuk yang terstruktur (sembarang), sehingga data tersebut masih perlu dilakukan \"Preprocessing Text\". Preprocessing Text adalah proses yang mengubah sebuah data yang tidak beraturan menjadi data yang terstruktur. Preprocessing text tidak memiliki tahapan yang pasti, dikarenakan jenis data yang berbeda membutuhkan aturan pemrosesan yang berbeda. Namun, tahapan yang umumnya digunakan pada pemrosesan teks adalah Parsing, Tokenizer (memecah data menjadi kalimat-kalimat), Stopword Removal (menghilangkan kata-kata yang tidak memiliki informasi penting), dan Stemming (menghilangkan imbuhan pada kata).","title":"Preprocessing Text"},{"location":"preprocessing/#melakukan-preprocessing-text","text":"","title":"Melakukan Preprocessing Text"},{"location":"preprocessing/#library-yang-dibutuhkan","text":"Library yang dibutuhkan pada preprocessing text kali ini adalah: - Sastrawi (untuk teks Bahasa Indonesia) - Requests-HTML (jika menggunakan library ini untuk crawling) - NLTK (untuk tokenizer dan stopword removal jika teks berbahasa Inggris) - Scikit-learn","title":"Library yang Dibutuhkan"},{"location":"preprocessing/#menyiapkan-dokumen","text":"Langkah pertama untuk melakukan pemrosesan teks adalah memanggil data dari file atau hasil crawling. Memanggil file eksternal textfile = codecs . open ( \"filename.txt\" , 'r' , 'utf-8' ) news = \"\" for line in textfile : news = news + line Menggunakan hasil crawler yang tersedia from requests_html import HTMLSession session = HTMLSession () r = session . get ( 'link-of-website' ) articles = r . html . find ( 'class' ) for item in articles : newsitem = item . find ( 'sub-class' , first = True ) news = newsitem . text print ( news )","title":"Menyiapkan Dokumen"},{"location":"preprocessing/#tokenizing","text":"Langkah selanjutnya adalah melakukan tokenizing. Tokenizing adalah membagi sebuah dokumen teks menjadi kalimat-kalimat dan menyimpannya ke dalam sebuah list. doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( news ) Maka hasilnya akan menjadi seperti berikut ini:","title":"Tokenizing"},{"location":"preprocessing/#regular-expression","text":"Langkah ini untuk menghilangkan angka dan elemen Python \"\\n\" pada teks dokumen. Baris kode di bawah ini untuk menghilangkan digit angka pada dokumen: import re dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) print ( dokumenre ) Setelah menghilangkan angka, maka langkah selanjutnya menghilangkan elemen \"\\n\" pada list kalimat dokumen. Baris kode di bawah ini untuk menghilangkan elemen tersebut: dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen )","title":"Regular Expression"},{"location":"preprocessing/#menghilangkan-stopword","text":"Stopword adalah kata-kata yang diabaikan dalam pemrosesan teks, karena stopword adalah kata-kata yang tidak memberikan informasi penting. Contoh dari stopword pada Bahasa Indonesia adalah kata penghubung seperti \"atau\", \"tapi\", \"dan\", dan lain sebagainya. Untuk menghilangkan stopword Bahasa Indonesia, library Python yang digunakan adalah Sastrawi. Dengan mengurangi stopword, maka jumlah kata pada dokumen akan lebih sedikit (dan jumlah fitur juga lebih sedikit) untuk pemrosesan teks. from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop )","title":"Menghilangkan Stopword"},{"location":"preprocessing/#stemming","text":"Stemming adalah proses untuk mentransformasikan sebuah kata dengan menghilangkan imbuhan (seperti \"me-\", \"-kan\") dan menjadi sebuah kata dasar. Misalnya, kata \"mengajarkan\" menjadi kata dasar \"ajar\". Untuk teks Bahasa Indonesia, proses stemming menggunakan Sastrawi: import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstop : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Langkah selanjutnya adalah menghilangkan tanda baca dalam kalimat: from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () dokumenstop = [] for i in dokumen : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstop . append ( output ) print ( dokumenstop )","title":"Stemming"},{"location":"preprocessing/#matriks-tf-idf","text":"Term Frequency-Inverse Document Frequency (TF-IDF) adalah matriks yang mengukur seberapa penting istilah atau fitur pada suatu dokumen. Matriks TF-IDF ini digunakan untuk pemodelan topik.","title":"Matriks TF-IDF"},{"location":"preprocessing/#vector-space-model","text":"Vector space model adalah from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer cv = CountVectorizer () cv_matrix = cv . fit_transform ( dokumen ) a = cv_matrix . toarray () bag = cv . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () matrik_vsm [ 0 ] Mendapatkan nama fitur: a = cv . get_feature_names () print ( len ( matrik_vsm [:, 1 ])) Matriks VSM: import pandas as pd dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ])","title":"Vector Space Model"},{"location":"preprocessing/#matriks-tf-idf_1","text":"from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( cv . fit_transform ( dokumenstem )) . toarray () dfb = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ])","title":"Matriks TF-IDF"}]}