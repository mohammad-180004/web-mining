{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Web Mining \u00b6 Profil Singkat \u00b6 Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Beranda"},{"location":"#selamat-datang-di-halaman-web-mining","text":"","title":"Selamat Datang di Halaman Web Mining"},{"location":"#profil-singkat","text":"Nama : Momohammad Haekal NIM : 180411100004 Kelas : Web Mining 8A Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Profil Singkat"},{"location":"crawling/","text":"Crawling \u00b6 Pengertian dari Crawling \u00b6 Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler Web Crawling dengan Requests-HTML \u00b6 Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin. 1. Memasang dan Memulai Library Requests-HTML \u00b6 Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession () 2. Memulai Crawling \u00b6 Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro. 3. Menyimpan Hasil Crawlin ke dalam File \u00b6 Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close () Web Crawling dengan Scrapy \u00b6 Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah: 1. Mempersiapkan Library Scrapy \u00b6 Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy 2. Melakukan Scraping pada Website \u00b6 Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja. Scrapy Shell \u00b6 Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy. Memulai Crawler \u00b6 Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah: Mengekstrak Konten dari Berita \u00b6 3. Membuat Spider Kustom \u00b6 4. Mengekspor Hasil Crawling \u00b6","title":"Web Crawling"},{"location":"crawling/#crawling","text":"","title":"Crawling"},{"location":"crawling/#pengertian-dari-crawling","text":"Crawling adalah suatu proses untuk mengambil dan mengunduh data dari database pada suatu halaman. Web crawler","title":"Pengertian dari Crawling"},{"location":"crawling/#web-crawling-dengan-requests-html","text":"Requests-HTML adalah library yang digunakan untuk melakukan crawling web semudah dan seintuitif mungkin.","title":"Web Crawling dengan Requests-HTML"},{"location":"crawling/#1-memasang-dan-memulai-library-requests-html","text":"Untuk memasang library ini, bisa menggunakan PyPI dengan perintah: pip install requests-html Setelah memasang library ini, langkah selanjutnya adalah mengimpor library ke dalam program dengan kode: from requests_html import HTMLSession session = HTMLSession ()","title":"1. Memasang dan Memulai Library Requests-HTML"},{"location":"crawling/#2-memulai-crawling","text":"Untuk memilih data yang akan di-crawl, cara yang digunakan sama seperti pada Scrapy. Class yang digunakan adalah: entry-content dan entry-content-single Tautan dapat disimpan dalam sebuah variabel agar memudahkan saat menulis kode. r = session . get ( 'https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro' ) articles = r . html . find ( 'div.entry-content' ) for item in articles : newsitem = item . find ( 'div.entry-content-single' , first = True ) news = newsitem . text print ( news ) Maka hasilnya akan seperti ini: Selama tahun 2019, Satuan Reskrim Polres Bojonegoro menangani sejumlah kasus korupsi yang telah dilanjutka. Ke ranah persidangan dan beberapa lainnya sudah menjalani hukuman setelah diputuskan bersalah oleh Pengadilan Negeri Bojonegoro, hal ini disampaikan oleh Kapolres Bojonegoro, AKBP M Budi Hendrawan, saat menyampaikan rilis akhir tahun 2019. Senin (30/12/19). Dalam paparanyya Kapolres Bojonegoro juga menyampaikan bahwa untuk penanganan kasus korupsi, penyidik memang butuh waktu yang tidak sebentar karena harus mengumpulkan alat bukti yang cukup kuat untuk menetapkan seseorang jadi tersangka. Dari Data penanganan kasus korupsi ini pada tahun 2018 Polres menangani dua kasus korupsi yang sudah di lanjutkan ke kejaksaan, dan oada tahun 2019 meningkat menjadi 4 kasus korupsi yang ditangani penyidik Satuan Reskrim Polres Bojonegoro. Memang ada kenaikan dari dua menjadi empat tahun 2019 ini, kata AKBP M Budi Hendrawan. Adapun rincian di tahun 2018 ada 8 kasus naik ke sidik menjadi 3 kasus serta tiga tersangka dengan kerugian negara Rp 631.260.625 dengan pengembalian kerugian Rp 301.935.000 sementara di tahun 2019 ini Polres Bojonegoro menangani 22 kasus dengan naik sidik 4 kasus dengan kerugian negara Rp 1.498.184.634 Dengan pengembalian kerugian Rp 461.898.213. Adapun rata rata tersangka dalam kasus koruosi ini adalah Kepala Desa yang menyalahgunakan wewenang dalam penggunaan dana Desa dan Anggaran Dana Desa. Dengan adanya kasus korupsi ini, perlu adanya pengawasan serta juga perhatian khususnya bagi pengelolaan Dana Desa, agar para pelaksana kegiatan bisa melaksanakan kegiatan dengan baik dan benar. Kami berharap tahun 2020 sudah tidak ada lagi korupsi di Bojonegoro, semuanya kerja dengan baik dan sesuai aturan pelaksanaan, Pungkas Kapolres Bojonegoro.","title":"2. Memulai Crawling"},{"location":"crawling/#3-menyimpan-hasil-crawlin-ke-dalam-file","text":"Untuk menyimpan hasil crawling dari Requests-HTML, maka menggunakan perintah open untuk menyimpan. Parameter pertama adalah nama file dan parameter kedua adalah mode (menggunakan \"w\" untuk menulis ke dalam file). result = open ( \"crawling.txt\" , \"w\" ) result = write ( news ) result . close ()","title":"3. Menyimpan Hasil Crawlin ke dalam File"},{"location":"crawling/#web-crawling-dengan-scrapy","text":"Scrapy adalah framework dari Python untuk web scraping (crawlin) dalam skala besar. Scrapy memberikan Anda semua alat yang Anda butuhkan untuk mengekstrak data dari situs web secara efisien, memprosesnya seperti yang Anda inginkan, dan menyimpannya dalam struktur dan format pilihan Anda. Berikut ini adalah cara meng-crawling sebuah website menggunakan library Python, yaitu scrapy. Langkah-langkahnya adalah:","title":"Web Crawling dengan Scrapy"},{"location":"crawling/#1-mempersiapkan-library-scrapy","text":"Langkah pertama adalah mempersiapkan library Scrapy. Scrapy ini dapat berjalan pada Python 2 dan 3 serta Anaconda. Untuk melakukan pemasangan ada dua cara: Metode Anaconda: conda install -c conda-forge scrapy Metode PyPI pip install scrapy","title":"1. Mempersiapkan Library Scrapy"},{"location":"crawling/#2-melakukan-scraping-pada-website","text":"Pada kali ini, website yang akan di-crawl adalah website berita SuaraBojonegoro. Anda bisa menggunakan website apa saja.","title":"2. Melakukan Scraping pada Website"},{"location":"crawling/#scrapy-shell","text":"Scrapy shell adalah antarmuka yang digunakan untuk berinteraksi dengan library Scrapy. Untuk memulai scrapy shell, masukkan perintah berikut ke dalam Command Prompt: scrapy shell Setelah berhasil menjalankan scrapy shell, langkah selanjutnya adalah mempersiapkan halaman yang akan di-crawl oleh scrapy.","title":"Scrapy Shell"},{"location":"crawling/#memulai-crawler","text":"Crawler membutuhkan titik awal untuk memulai crawling (mengunduh) konten dari situs web. Tautan yang digunakana pada kali ini adalah:","title":"Memulai Crawler"},{"location":"crawling/#mengekstrak-konten-dari-berita","text":"","title":"Mengekstrak Konten dari Berita"},{"location":"crawling/#3-membuat-spider-kustom","text":"","title":"3. Membuat Spider Kustom"},{"location":"crawling/#4-mengekspor-hasil-crawling","text":"","title":"4. Mengekspor Hasil Crawling"},{"location":"evaluasi/","text":"Evaluasi \u00b6","title":"LSA Evaluation"},{"location":"evaluasi/#evaluasi","text":"","title":"Evaluasi"},{"location":"modelling/","text":"Modelling \u00b6","title":"Topic Modelling"},{"location":"modelling/#modelling","text":"","title":"Modelling"},{"location":"preprocessing/","text":"Preprocessing \u00b6","title":"Preprocessing Text"},{"location":"preprocessing/#preprocessing","text":"","title":"Preprocessing"}]}